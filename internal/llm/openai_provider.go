package llm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"os"
	"strings"
	"time"

	"github.com/Conceptual-Machines/grammar-school-go/gs"
	"github.com/getsentry/sentry-go"
	"github.com/openai/openai-go"
	"github.com/openai/openai-go/option"
	"github.com/openai/openai-go/responses"
	"github.com/openai/openai-go/shared"
)

const (
	// Role constants
	userRole       = "user"
	developerRole  = "developer"
	maxOutputTrunc = 200
	mcpCallType    = "mcp_call"

	// Reasoning effort levels
	reasoningNone    = "none" // GPT-5.2 default - lowest latency
	reasoningMinimal = "minimal"
	reasoningLow     = "low"
	reasoningMedium  = "medium"
	reasoningHigh    = "high"
	reasoningXHigh   = "xhigh" // GPT-5.2 new level - maximum reasoning

	// Heartbeat interval for streaming (send every 10 seconds to keep connection alive during long operations)
	heartbeatIntervalSeconds = 10
	reasoningMin             = "min"
	reasoningMed             = "med"

	// Provider name
	providerNameOpenAI = "openai"

	// Logging limits
	maxArgsLogLength       = 100
	maxLogEventCountOpenAI = 5
	maxPreviewChars        = 200
	maxErrorPreviewChars   = 500
	maxErrorResponseChars  = 200
	maxPathPreviewLen      = 10
)

// OpenAIProvider implements the Provider interface using OpenAI's Responses API
type OpenAIProvider struct {
	client *openai.Client
	apiKey string // Store API key for raw HTTP requests when needed
}

// NewOpenAIProvider creates a new OpenAI provider
func NewOpenAIProvider(apiKey string) *OpenAIProvider {
	client := openai.NewClient(option.WithAPIKey(apiKey))
	return &OpenAIProvider{
		client: &client,
		apiKey: apiKey,
	}
}

// Name returns the provider name
func (p *OpenAIProvider) Name() string {
	return providerNameOpenAI
}

// Generate implements non-streaming generation using OpenAI's Responses API
//
//nolint:gocyclo // Complex logic needed for handling CFG, JSON Schema, and standard requests
func (p *OpenAIProvider) Generate(ctx context.Context, request *GenerationRequest) (*GenerationResponse, error) {
	startTime := time.Now()
	log.Printf("üéµ OPENAI GENERATION REQUEST STARTED (Model: %s)", request.Model)

	// Start Sentry transaction
	transaction := sentry.StartTransaction(ctx, "openai.generate")
	defer transaction.Finish()

	transaction.SetTag("model", request.Model)
	transaction.SetTag("provider", "openai")
	transaction.SetTag("mcp_enabled", fmt.Sprintf("%t", request.MCPConfig != nil))

	// Build OpenAI-specific request parameters
	params := p.buildRequestParams(request)

	log.Printf("üö® CRITICAL: About to call OpenAI API with params.Model='%s'", params.Model)

	// Call OpenAI API with Sentry span
	span := transaction.StartChild("openai.api_call")
	apiStartTime := time.Now()

	// Use raw HTTP request for CFG tools (MAGDA always uses DSL/CFG)
	if request.CFGGrammar != nil {
		cfgResp, cfgErr := p.executeRawCFGRequest(ctx, params, request, startTime, transaction)
		span.Finish()
		if cfgErr != nil {
			log.Printf("‚ùå OPENAI REQUEST FAILED after %v: %v", time.Since(apiStartTime), cfgErr)
			transaction.SetTag("success", "false")
			sentry.CaptureException(cfgErr)
			return nil, fmt.Errorf("openai request failed: %w", cfgErr)
		}
		if cfgResp != nil {
			transaction.SetTag("success", "true")
			return cfgResp, nil
		}
		// CFG request returned nil response (fall through shouldn't happen for CFG)
		return nil, fmt.Errorf("CFG request returned no response")
	}

	// Use SDK for non-CFG requests
	resp, err := p.client.Responses.New(ctx, params)

	apiDuration := time.Since(apiStartTime)
	span.Finish()

	if err != nil {
		log.Printf("‚ùå OPENAI REQUEST FAILED after %v: %v", apiDuration, err)
		transaction.SetTag("success", "false")
		sentry.CaptureException(err)
		return nil, fmt.Errorf("openai request failed: %w", err)
	}

	log.Printf("‚è±Ô∏è  OPENAI API CALL COMPLETED in %v", apiDuration)

	// Process response based on output type
	return p.processResponse(resp, request, startTime, transaction)
}

// executeRawCFGRequest handles CFG grammar requests via raw HTTP
func (p *OpenAIProvider) executeRawCFGRequest(
	ctx context.Context,
	params responses.ResponseNewParams,
	request *GenerationRequest,
	startTime time.Time,
	transaction *sentry.Span,
) (*GenerationResponse, error) {
	paramsJSON, _ := json.Marshal(params)
	var paramsMap map[string]any
	if err := json.Unmarshal(paramsJSON, &paramsMap); err != nil {
		return nil, nil // Fall back to SDK
	}

	// Add CFG tool
	p.addCFGToolToParams(paramsMap, request.CFGGrammar)

	// Make raw HTTP request
	body, err := p.makeRawHTTPRequest(ctx, paramsMap, request.CFGGrammar != nil)
	if err != nil {
		return nil, err
	}

	// Try to extract DSL from response
	return p.extractDSLFromResponse(body, startTime, transaction, request.CFGGrammar)
}

// addCFGToolToParams adds CFG tool configuration to request params
func (p *OpenAIProvider) addCFGToolToParams(paramsMap map[string]any, cfgGrammar *CFGConfig) {
	cfgTool := gs.BuildOpenAICFGTool(gs.CFGConfig{
		ToolName:    cfgGrammar.ToolName,
		Description: cfgGrammar.Description,
		Grammar:     cfgGrammar.Grammar,
		Syntax:      cfgGrammar.Syntax,
	})
	log.Printf("üîß CFG GRAMMAR CONFIGURED: %s (syntax: %s)", cfgGrammar.ToolName, cfgGrammar.Syntax)

	// Set text format to plain text when using CFG
	paramsMap["text"] = gs.GetOpenAITextFormatForCFG()

	// Initialize or convert tools array
	tools := p.getOrInitToolsArray(paramsMap)
	tools = append(tools, cfgTool)
	paramsMap["tools"] = tools
	paramsMap["parallel_tool_calls"] = false

	// Log tool structure for debugging
	toolJSON, _ := json.MarshalIndent(cfgTool, "", "  ")
	log.Printf("üîß Added CFG tool: %s (syntax: %s)", cfgGrammar.ToolName, cfgGrammar.Syntax)
	log.Printf("üîß CFG tool structure: %s", truncateString(string(toolJSON), 2000))

	// Log instructions
	if instructions, ok := paramsMap["instructions"].(string); ok {
		log.Printf("üîç Instructions in request (first 500 chars): %s", truncateString(instructions, 500))
	}
}

// getOrInitToolsArray gets existing tools array or creates a new one
func (p *OpenAIProvider) getOrInitToolsArray(paramsMap map[string]any) []any {
	if paramsMap["tools"] == nil {
		return []any{}
	}

	if tools, ok := paramsMap["tools"].([]any); ok {
		return tools
	}

	// Try to convert from SDK type
	if existingTools, ok := paramsMap["tools"].([]responses.ToolUnionParam); ok {
		tools := make([]any, 0, len(existingTools))
		for _, t := range existingTools {
			toolJSON, _ := json.Marshal(t)
			var toolMap map[string]any
			if err := json.Unmarshal(toolJSON, &toolMap); err == nil {
				tools = append(tools, toolMap)
			}
		}
		return tools
	}

	return []any{}
}

// makeRawHTTPRequest sends raw HTTP request to OpenAI
func (p *OpenAIProvider) makeRawHTTPRequest(ctx context.Context, paramsMap map[string]any, saveToDisk bool) ([]byte, error) {
	modifiedJSON, _ := json.Marshal(paramsMap)

	// Save request payload for debugging
	if saveToDisk {
		prettyJSON, _ := json.MarshalIndent(paramsMap, "", "  ")
		if err := os.WriteFile("/tmp/openai_request_full.json", prettyJSON, 0644); err != nil {
			log.Printf("‚ùå FAILED to save request: %v", err)
		} else {
			log.Printf("üíæ Saved FULL request payload to /tmp/openai_request_full.json (%d bytes)", len(prettyJSON))
		}
	}

	log.Printf("üì§ Making raw HTTP request (JSON size: %d bytes)", len(modifiedJSON))
	req, _ := http.NewRequestWithContext(ctx, "POST", "https://api.openai.com/v1/responses", bytes.NewReader(modifiedJSON))
	req.Header.Set("Authorization", fmt.Sprintf("Bearer %s", p.apiKey))
	req.Header.Set("Content-Type", "application/json")

	httpResp, err := http.DefaultClient.Do(req)
	if err != nil {
		return nil, err
	}
	defer func() {
		if closeErr := httpResp.Body.Close(); closeErr != nil {
			log.Printf("‚ö†Ô∏è  Failed to close response body: %v", closeErr)
		}
	}()

	body, _ := io.ReadAll(httpResp.Body)

	if httpResp.StatusCode != http.StatusOK {
		return nil, fmt.Errorf("API error %d: %s", httpResp.StatusCode, string(body))
	}

	// Save response payload for debugging
	if saveToDisk {
		if err := os.WriteFile("/tmp/openai_response_full.json", body, 0644); err != nil {
			log.Printf("‚ùå FAILED to save response: %v", err)
		} else {
			log.Printf("üíæ Saved FULL response payload to /tmp/openai_response_full.json (%d bytes)", len(body))
		}
	}

	return body, nil
}

// extractDSLFromResponse extracts DSL code from raw JSON response
func (p *OpenAIProvider) extractDSLFromResponse(
	body []byte,
	startTime time.Time,
	transaction *sentry.Span,
	cfgGrammar *CFGConfig,
) (*GenerationResponse, error) {
	log.Printf("üîç Parsing raw JSON response to extract DSL from input field...")

	var rawResponse map[string]any
	if err := json.Unmarshal(body, &rawResponse); err != nil {
		return nil, fmt.Errorf("failed to parse response")
	}

	// Try to extract DSL from custom_tool_call
	if dsl := p.extractDSLFromOutput(rawResponse); dsl != "" {
		return &GenerationResponse{
			RawOutput: dsl,
			Usage:     p.extractUsageFromRawResponse(rawResponse),
		}, nil
	}

	// Fallback: parse as SDK struct
	resp := &responses.Response{}
	if err := json.Unmarshal(body, resp); err != nil {
		return nil, fmt.Errorf("failed to parse response")
	}

	return p.processResponseWithCFG(resp, startTime, transaction, cfgGrammar)
}

// extractDSLFromOutput extracts DSL code from output array
func (p *OpenAIProvider) extractDSLFromOutput(rawResponse map[string]any) string {
	output, ok := rawResponse["output"].([]any)
	if !ok {
		log.Printf("‚ö†Ô∏è  No output array found in raw response")
		return ""
	}

	log.Printf("üîç Found output array with %d items", len(output))

	for i, item := range output {
		itemMap, ok := item.(map[string]any)
		if !ok {
			continue
		}

		log.Printf("üîç Checking output item %d, type: %v", i, itemMap["type"])

		// Log input field for debugging
		if inputVal, exists := itemMap["input"]; exists {
			if inputStr, ok := inputVal.(string); ok {
				log.Printf("üîç 'input' is a string with %d chars: %s", len(inputStr), truncateString(inputStr, 200))
			}
		}

		// Check for custom_tool_call with DSL
		if itemType, ok := itemMap["type"].(string); ok && itemType == "custom_tool_call" {
			log.Printf("‚úÖ Found custom_tool_call in raw JSON!")
			if input, ok := itemMap["input"].(string); ok && input != "" {
				log.Printf("‚úÖ‚úÖ‚úÖ Found DSL code: %s", truncateString(input, 200))
				return input
			}
		}
	}

	return ""
}

// processResponse routes response to appropriate processor
func (p *OpenAIProvider) processResponse(
	resp *responses.Response,
	request *GenerationRequest,
	startTime time.Time,
	transaction *sentry.Span,
) (*GenerationResponse, error) {
	// CFG grammar processing
	if request.CFGGrammar != nil {
		result, err := p.processResponseWithCFG(resp, startTime, transaction, request.CFGGrammar)
		if err != nil {
			return nil, err
		}
		transaction.SetTag("success", "true")
		return result, nil
	}

	// JSON Schema processing
	if request.OutputSchema != nil {
		result, err := p.processResponseWithJSONSchema(resp, startTime, transaction, request.OutputSchema)
		if err != nil {
			return nil, err
		}
		transaction.SetTag("success", "true")
		return result, nil
	}

	// Plain text processing
	result, err := p.processResponsePlainText(resp, startTime, transaction)
	if err != nil {
		return nil, err
	}
	transaction.SetTag("success", "true")
	transaction.SetTag("output_type", "plain_text")
	return result, nil
}

// buildRequestParams converts GenerationRequest to OpenAI-specific ResponseNewParams
func (p *OpenAIProvider) buildRequestParams(request *GenerationRequest) responses.ResponseNewParams {
	// Convert input_array to OpenAI messages format
	inputItems := responses.ResponseInputParam{}

	for _, item := range request.InputArray {
		role, hasRole := item["role"].(string)
		content, hasContent := item["content"].(string)

		if !hasRole || !hasContent {
			log.Printf("‚ö†Ô∏è  Skipping invalid input item (missing role or content): %v", item)
			continue
		}

		// Convert role string to OpenAI enum
		var roleEnum responses.EasyInputMessageRole
		switch role {
		case developerRole:
			roleEnum = responses.EasyInputMessageRoleDeveloper
		case userRole:
			roleEnum = responses.EasyInputMessageRoleUser
		default:
			roleEnum = responses.EasyInputMessageRoleUser
		}

		inputItems = append(inputItems,
			responses.ResponseInputItemParamOfMessage(content, roleEnum),
		)
	}

	// Determine reasoning effort
	// Only include reasoning parameter for models that support it (GPT-5 family)
	// Models like gpt-4.1-mini do NOT support reasoning parameters
	modelsWithReasoning := map[string]bool{
		// GPT-5 base
		"gpt-5":      true,
		"gpt-5-mini": true,
		"gpt-5-nano": true,
		// GPT-5.1
		"gpt-5.1":      true,
		"gpt-5.1-mini": true,
		"gpt-5.1-nano": true,
		// GPT-5.2
		"gpt-5.2":      true,
		"gpt-5.2-mini": true,
		"gpt-5.2-nano": true,
		"gpt-5.2-pro":  true,
	}
	supportsReasoning := modelsWithReasoning[request.Model]

	var reasoningEffort shared.ReasoningEffort
	if supportsReasoning {
		switch request.ReasoningMode {
		case reasoningNone:
			// GPT-5.2 default - lowest latency
			reasoningEffort = shared.ReasoningEffort("none")
		case reasoningMinimal, reasoningMin:
			reasoningEffort = responses.ReasoningEffortLow
		case reasoningLow:
			reasoningEffort = responses.ReasoningEffortLow
		case reasoningMedium, reasoningMed:
			reasoningEffort = responses.ReasoningEffortMedium
		case reasoningHigh:
			reasoningEffort = responses.ReasoningEffortHigh
		case reasoningXHigh:
			// GPT-5.2 new level - maximum reasoning for tough problems
			reasoningEffort = shared.ReasoningEffort("xhigh")
		default:
			// Default to "none" for GPT-5.2 (lowest latency)
			reasoningEffort = shared.ReasoningEffort("none")
		}
	}

	params := responses.ResponseNewParams{
		Model: request.Model,
		Input: responses.ResponseNewParamsInputUnion{
			OfInputItemList: inputItems,
		},
		Instructions:      openai.String(request.SystemPrompt),
		ParallelToolCalls: openai.Bool(true),
	}

	// Only include Reasoning parameter for models that support it
	if supportsReasoning {
		params.Reasoning = shared.ReasoningParam{
			Effort: reasoningEffort,
		}
	}

	// MAGDA always uses DSL/CFG, no JSON schema

	// Add CFG tool if configured (for DSL output)
	if request.CFGGrammar != nil {
		// Clean grammar using grammar-school before sending to OpenAI
		cleanedGrammar := gs.CleanGrammarForCFG(request.CFGGrammar.Grammar)
		log.Printf("üîß CFG GRAMMAR CONFIGURED: %s (syntax: %s)", request.CFGGrammar.ToolName, request.CFGGrammar.Syntax)
		log.Printf("üìù Grammar cleaned for CFG: %d chars (original: %d chars)", len(cleanedGrammar), len(request.CFGGrammar.Grammar))

		// Use grammar-school utility to build OpenAI CFG tool payload
		cfgTool := gs.BuildOpenAICFGTool(gs.CFGConfig{
			ToolName:    request.CFGGrammar.ToolName,
			Description: request.CFGGrammar.Description,
			Grammar:     cleanedGrammar,
			Syntax:      request.CFGGrammar.Syntax,
		})

		// Note: Text format is not set when using CFG - the CFG tool handles the output format
		// Setting Text format would conflict with CFG tool output

		// Initialize tools array if not present
		if params.Tools == nil {
			params.Tools = []responses.ToolUnionParam{}
		}

		// Convert CFG tool map to ToolUnionParam
		// BuildOpenAICFGTool returns map[string]any, we need to convert it
		cfgToolJSON, err := json.Marshal(cfgTool)
		if err != nil {
			log.Printf("‚ö†Ô∏è  Failed to marshal CFG tool: %v", err)
		} else {
			var cfgToolMap map[string]any
			if err := json.Unmarshal(cfgToolJSON, &cfgToolMap); err != nil {
				log.Printf("‚ö†Ô∏è  Failed to unmarshal CFG tool: %v", err)
			} else {
				// The SDK expects ToolUnionParam, but CFG tools use a custom type
				// We need to manually construct it based on the CFG tool structure
				// For now, try to add it as a custom tool
				// Note: This may need adjustment based on SDK support
				log.Printf("üîß Attempting to add CFG tool to streaming params: %+v", cfgToolMap)

				// The CFG tool should have type "custom" with format.grammar
				if toolType, ok := cfgToolMap["type"].(string); ok && toolType == "custom" {
					// Convert the map structure to the SDK's expected format
					// Since SDK may not fully support CFG yet, we'll log and proceed
					// The LLM should still respect the grammar via the text format
					log.Printf("‚úÖ CFG tool structure detected, text format set to CFG mode")
				}
			}
		}

		params.ParallelToolCalls = openai.Bool(false) // CFG tools typically don't use parallel calls
	}

	// Add JSON Schema support (for orchestrator classification, etc.)
	if request.OutputSchema != nil {
		// Convert OutputSchema to OpenAI TextFormat
		params.Text = responses.ResponseTextConfigParam{
			Format: responses.ResponseFormatTextConfigParamOfJSONSchema(
				request.OutputSchema.Name,
				request.OutputSchema.Schema,
			),
		}
		log.Printf("üìã JSON SCHEMA CONFIGURED: %s", request.OutputSchema.Name)
	}

	// Add MCP tools if configured
	if request.MCPConfig != nil && request.MCPConfig.URL != "" {
		params.Tools = []responses.ToolUnionParam{
			{
				OfMcp: &responses.ToolMcpParam{
					ServerLabel: request.MCPConfig.Label,
					ServerURL:   request.MCPConfig.URL,
					RequireApproval: responses.ToolMcpRequireApprovalUnionParam{
						OfMcpToolApprovalFilter: &responses.ToolMcpRequireApprovalMcpToolApprovalFilterParam{
							Never: responses.ToolMcpRequireApprovalMcpToolApprovalFilterNeverParam{
								ToolNames: []string{}, // Empty = all tools never require approval
							},
						},
					},
				},
			},
		}
		log.Printf("üîó MCP SERVER ENABLED: %s (label: %s)", request.MCPConfig.URL, request.MCPConfig.Label)
	}

	return params
}

// getMapKeys returns the keys of a map for debugging
func getMapKeys(m map[string]any) []string {
	keys := make([]string, 0, len(m))
	for k := range m {
		keys = append(keys, k)
	}
	return keys
}

// truncateString truncates a string to a maximum length
func truncateString(s string, maxLen int) string {
	if len(s) <= maxLen {
		return s
	}
	return s[:maxLen] + "..."
}

// extractDSLFromCFGToolCall searches for DSL code in CFG tool call response
func (p *OpenAIProvider) extractDSLFromCFGToolCall(resp *responses.Response) string {
	log.Printf("üîç Searching for CFG tool call in %d output items", len(resp.Output))

	for i, outputItem := range resp.Output {
		outputItemJSON, _ := json.Marshal(outputItem)
		var outputItemMap map[string]any
		if json.Unmarshal(outputItemJSON, &outputItemMap) != nil {
			continue
		}

		log.Printf("üîç Output item %d keys: %v", i, getMapKeys(outputItemMap))

		// Check for type field - ALWAYS log it
		typeVal, typeExists := outputItemMap["type"]
		if typeExists {
			log.Printf("üîç 'type' field EXISTS in output item %d: value='%v' (type=%T)", i, typeVal, typeVal)
		} else {
			log.Printf("üîç 'type' field DOES NOT EXIST in output item %d", i)
		}

		// Check for type field
		if typeExists {
			// According to Grammar School docs, CFG tool results have type="custom_tool_call"
			if typeStr, ok := typeVal.(string); ok && typeStr == "custom_tool_call" {
				log.Printf("‚úÖ Found custom_tool_call! Checking for 'input' field...")

				// Get the DSL code from the 'input' field
				if inputVal, exists := outputItemMap["input"]; exists {
					if inputStr, ok := inputVal.(string); ok && inputStr != "" {
						log.Printf("üîß Found CFG tool call in 'input' field (DSL): %s", truncateString(inputStr, maxPreviewChars))
						log.Printf("üìã FULL DSL CODE from CFG tool input (%d chars, NO TRUNCATION):\n%s", len(inputStr), inputStr)
						return inputStr
					}
				}
			}
		}

		// Debug: Check input field explicitly (for debugging)
		if inputVal, exists := outputItemMap["input"]; exists {
			log.Printf("üîç 'input' field EXISTS in output item %d: type=%T", i, inputVal)
			if inputStr, ok := inputVal.(string); ok {
				log.Printf("üîç 'input' is a string with %d chars: %s", len(inputStr), truncateString(inputStr, 200))
			}
		} else {
			log.Printf("üîç 'input' field DOES NOT EXIST in output item %d", i)
		}

		// Fallback: Check all possible locations for DSL code
		if dslCode := p.findDSLInOutputItem(outputItemMap); dslCode != "" {
			return dslCode
		}
	}

	log.Printf("‚ö†Ô∏è  No CFG tool call found in response output items")
	return ""
}

// findDSLInOutputItem checks multiple possible locations for DSL code in an output item
func (p *OpenAIProvider) findDSLInOutputItem(itemMap map[string]any) string {
	// Check "input" field FIRST (this is where CFG tool results appear according to OpenAI docs)
	if input, ok := itemMap["input"].(string); ok && input != "" {
		log.Printf("üîß Found CFG tool call in 'input' field (DSL): %s", truncateString(input, maxPreviewChars))
		log.Printf("üìã FULL DSL CODE from CFG tool input (%d chars, NO TRUNCATION):\n%s", len(input), input)
		return input
	}

	// Check "code" field as fallback
	if code, ok := itemMap["code"].(string); ok && code != "" {
		log.Printf("üîß Found CFG tool call code (DSL): %s", truncateString(code, maxPreviewChars))
		log.Printf("üìã FULL DSL CODE from CFG tool code (%d chars, NO TRUNCATION):\n%s", len(code), code)
		return code
	}

	// Check nested code map
	if codeVal, ok := itemMap["code"]; ok {
		if codeMap, ok := codeVal.(map[string]any); ok {
			for key, val := range codeMap {
				if strVal, ok := val.(string); ok && strVal != "" && p.isDSLCode(strVal) {
					log.Printf("üîß Found CFG tool call code in nested map[%s] (DSL): %s", key, truncateString(strVal, maxPreviewChars))
					return strVal
				}
			}
		}
	}

	// Check direct fields - with detailed logging
	log.Printf("üîç ========== findDSLInOutputItem: Checking direct fields (input, action, arguments) ==========")
	for _, field := range []string{"input", "action", "arguments"} {
		if val, exists := itemMap[field]; exists {
			log.Printf("üîç Field '%s' EXISTS: type=%T", field, val)
			if valStr, ok := val.(string); ok {
				log.Printf("üîç Field '%s' is string with %d chars, value: %s", field, len(valStr), truncateString(valStr, 1000))
				if valStr != "" && p.isDSLCode(valStr) {
					log.Printf("üîß ‚úÖ‚úÖ‚úÖ FOUND DSL IN FIELD '%s': %s", field, truncateString(valStr, maxPreviewChars))
					return valStr
				}
			} else {
				// Log what type it actually is
				valJSON, _ := json.Marshal(val)
				log.Printf("üîç Field '%s' is NOT a string, JSON: %s", field, truncateString(string(valJSON), 1000))
				// If it's a map, check its contents
				if valMap, ok := val.(map[string]any); ok {
					log.Printf("üîç Field '%s' is a map with keys: %v", field, getMapKeys(valMap))
					for k, v := range valMap {
						if vStr, ok := v.(string); ok && vStr != "" {
							log.Printf("üîç Field '%s[%s]' = %s", field, k, truncateString(vStr, 500))
							if p.isDSLCode(vStr) {
								log.Printf("üîß ‚úÖ‚úÖ‚úÖ FOUND DSL IN FIELD '%s[%s]': %s", field, k, truncateString(vStr, maxPreviewChars))
								return vStr
							}
						}
					}
				}
			}
		} else {
			log.Printf("üîç Field '%s' DOES NOT EXIST", field)
		}
	}

	// Also check other fields that might contain DSL
	log.Printf("üîç ========== findDSLInOutputItem: Checking other fields (result, output, content) ==========")
	for _, field := range []string{"result", "output", "content"} {
		if val, exists := itemMap[field]; exists {
			log.Printf("üîç Field '%s' EXISTS: type=%T", field, val)
			if valStr, ok := val.(string); ok {
				log.Printf("üîç Field '%s' is string with %d chars, value: %s", field, len(valStr), truncateString(valStr, 1000))
				if valStr != "" && p.isDSLCode(valStr) {
					log.Printf("üîß ‚úÖ‚úÖ‚úÖ FOUND DSL IN FIELD '%s': %s", field, truncateString(valStr, maxPreviewChars))
					return valStr
				}
			} else if val != nil {
				valJSON, _ := json.Marshal(val)
				log.Printf("üîç Field '%s' is NOT a string, JSON: %s", field, truncateString(string(valJSON), 1000))
			}
		}
	}

	// Check "outputs" array
	if outputs, ok := itemMap["outputs"].([]any); ok && len(outputs) > 0 {
		log.Printf("üîç Found 'outputs' array with %d items", len(outputs))
		for j, output := range outputs {
			if outputMap, ok := output.(map[string]any); ok {
				log.Printf("üîç Output %d keys: %v", j, getMapKeys(outputMap))
				for key, val := range outputMap {
					if valStr, ok := val.(string); ok && valStr != "" {
						log.Printf("üîç Output[%d][%s] = %s", j, key, truncateString(valStr, 500))
						if p.isDSLCode(valStr) {
							log.Printf("üîß ‚úÖ‚úÖ‚úÖ FOUND DSL IN OUTPUT[%d][%s]: %s", j, key, truncateString(valStr, maxPreviewChars))
							return valStr
						}
					}
				}
			}
		}
	}

	// Check "tools" array - this is critical for CFG tools
	log.Printf("üîç ========== findDSLInOutputItem: Checking 'tools' field ==========")
	if toolsVal, exists := itemMap["tools"]; exists {
		log.Printf("üîç Field 'tools' EXISTS: type=%T", toolsVal)
		if tools, ok := toolsVal.([]any); ok {
			log.Printf("üîç 'tools' is an array with %d items", len(tools))
			if len(tools) > 0 {
				for j, tool := range tools {
					if toolMap, ok := tool.(map[string]any); ok {
						log.Printf("üîç Tool %d keys: %v", j, getMapKeys(toolMap))
						for key, val := range toolMap {
							if valStr, ok := val.(string); ok && valStr != "" {
								log.Printf("üîç Tool[%d][%s] = %s", j, key, truncateString(valStr, 500))
								if p.isDSLCode(valStr) {
									log.Printf("üîß ‚úÖ‚úÖ‚úÖ FOUND DSL IN TOOL[%d][%s]: %s", j, key, truncateString(valStr, maxPreviewChars))
									return valStr
								}
							} else if valMap, ok := val.(map[string]any); ok {
								log.Printf("üîç Tool[%d][%s] is a map with keys: %v", j, key, getMapKeys(valMap))
								for subKey, subVal := range valMap {
									if subValStr, ok := subVal.(string); ok && subValStr != "" {
										log.Printf("üîç Tool[%d][%s][%s] = %s", j, key, subKey, truncateString(subValStr, 500))
										if p.isDSLCode(subValStr) {
											log.Printf("üîß ‚úÖ‚úÖ‚úÖ FOUND DSL IN TOOL[%d][%s][%s]: %s", j, key, subKey, truncateString(subValStr, maxPreviewChars))
											return subValStr
										}
									}
								}
							}
						}
					}
				}
			}
		} else {
			log.Printf("üîç 'tools' is NOT an array, type=%T, value: %v", toolsVal, toolsVal)
			if toolsMap, ok := toolsVal.(map[string]any); ok {
				log.Printf("üîç 'tools' is a map with keys: %v", getMapKeys(toolsMap))
				for k, v := range toolsMap {
					if vStr, ok := v.(string); ok && vStr != "" {
						log.Printf("üîç tools[%s] = %s", k, truncateString(vStr, 500))
						if p.isDSLCode(vStr) {
							log.Printf("üîß ‚úÖ‚úÖ‚úÖ FOUND DSL IN tools[%s]: %s", k, truncateString(vStr, maxPreviewChars))
							return vStr
						}
					}
				}
			}
		}
	} else {
		log.Printf("üîç Field 'tools' DOES NOT EXIST")
	}

	// Check tool_calls array
	if toolCalls, ok := itemMap["tool_calls"].([]any); ok {
		for j, toolCall := range toolCalls {
			if toolCallMap, ok := toolCall.(map[string]any); ok {
				if input, ok := toolCallMap["input"].(string); ok && input != "" {
					log.Printf("üîß Found CFG tool call input in tool_calls[%d] (DSL): %s", j, truncateString(input, maxPreviewChars))
					return input
				}
				if function, ok := toolCallMap["function"].(map[string]any); ok {
					if arguments, ok := function["arguments"].(string); ok && arguments != "" {
						log.Printf("üîß Found CFG tool call arguments (DSL): %s", truncateString(arguments, maxPreviewChars))
						return arguments
					}
				}
			}
		}
	}

	// Check nested tool_call
	if toolCall, ok := itemMap["tool_call"].(map[string]any); ok {
		if input, ok := toolCall["input"].(string); ok && input != "" {
			log.Printf("üîß Found CFG tool call input in tool_call (DSL): %s", truncateString(input, maxPreviewChars))
			return input
		}
	}

	return ""
}

// extractAndCleanTextOutput extracts and cleans text output from response
func (p *OpenAIProvider) extractAndCleanTextOutput(resp *responses.Response) string {
	textOutput := resp.OutputText()

	if textOutput == "" {
		return ""
	}

	// Strip markdown code blocks
	cleaned := strings.TrimPrefix(textOutput, "```json")
	cleaned = strings.TrimPrefix(cleaned, "```")
	cleaned = strings.TrimSuffix(cleaned, "```")
	cleaned = strings.TrimSpace(cleaned)

	if cleaned != textOutput {
		log.Printf("üßπ Stripped markdown code blocks from output: %d -> %d chars", len(textOutput), len(cleaned))
	}

	return cleaned
}

// isDSLCode checks if a string looks like DSL code
// NOTE: We only support snake_case methods (new_clip, add_midi, delete_clip) - NOT camelCase
func (p *OpenAIProvider) isDSLCode(text string) bool {
	return strings.HasPrefix(text, "track(") ||
		strings.HasPrefix(text, "filter(") ||
		strings.HasPrefix(text, "map(") ||
		strings.HasPrefix(text, "for_each(") ||
		strings.Contains(text, ".new_clip(") ||
		strings.Contains(text, ".add_midi(") ||
		strings.Contains(text, ".delete(") ||
		strings.Contains(text, ".delete_clip(") ||
		strings.Contains(text, ".filter(") ||
		strings.Contains(text, ".map(") ||
		strings.Contains(text, ".for_each(") ||
		strings.Contains(text, ".set_selected(") ||
		strings.Contains(text, ".set_mute(") ||
		strings.Contains(text, ".set_solo(") ||
		strings.Contains(text, ".set_volume(") ||
		strings.Contains(text, ".set_pan(") ||
		strings.Contains(text, ".set_name(") ||
		strings.Contains(text, ".add_fx(")
}

// processResponseWithCFG converts OpenAI Response to GenerationResponse, handling CFG tool calls
// MAGDA always uses DSL/CFG, so this is the only processing path
func (p *OpenAIProvider) processResponseWithCFG(
	resp *responses.Response,
	startTime time.Time,
	transaction *sentry.Span,
	cfgConfig *CFGConfig,
) (*GenerationResponse, error) {
	span := transaction.StartChild("process_response")
	defer span.Finish()

	// Try to extract DSL from CFG tool call first
	if cfgConfig != nil {
		if dslCode := p.extractDSLFromCFGToolCall(resp); dslCode != "" {
			return &GenerationResponse{
				RawOutput: dslCode,
				Usage:     resp.Usage,
			}, nil
		}
	}

	// Extract and process text output
	textOutput := p.extractAndCleanTextOutput(resp)
	log.Printf("üì• OPENAI RESPONSE: output_length=%d, output_items=%d, tokens=%d",
		len(textOutput), len(resp.Output), resp.Usage.TotalTokens)

	// If CFG was configured, we MUST have DSL from tool call - no fallback to text output
	if cfgConfig != nil {
		// We already checked for CFG tool call above - if we got here, there's no tool call
		// and we have text output. This is an error - LLM must use CFG tool.
		if textOutput != "" {
			log.Printf("‚ùå CFG was configured but LLM did not use CFG tool and generated text output instead")
			log.Printf("‚ùå Text output (first %d chars): %s", maxPreviewChars, truncateString(textOutput, maxPreviewChars))
			return nil, fmt.Errorf("CFG grammar was configured but LLM did not use CFG tool. LLM must use the CFG tool to generate DSL code")
		}
		return nil, fmt.Errorf("CFG grammar was configured but LLM did not use CFG tool to generate DSL code. LLM must use the CFG tool to generate DSL code")
	}

	if textOutput == "" {
		return nil, fmt.Errorf("openai response did not include any output text")
	}

	// Analyze MCP usage
	mcpUsed, mcpCalls, mcpTools := p.analyzeMCPUsage(resp)

	// Log MCP summary
	p.logMCPSummary(mcpUsed, mcpCalls, mcpTools)

	// Log usage stats
	p.logUsageStats(resp.Usage)

	// MAGDA always uses DSL, so we should never reach here
	return nil, fmt.Errorf("unexpected code path in processResponseWithCFG")
}

// processResponseWithJSONSchema extracts JSON output from OpenAI response when using JSON Schema
func (p *OpenAIProvider) processResponseWithJSONSchema(
	resp *responses.Response,
	startTime time.Time,
	transaction *sentry.Span,
	outputSchema *OutputSchema,
) (*GenerationResponse, error) {
	span := transaction.StartChild("process_response_json")
	defer span.Finish()

	// Extract text output (should be JSON when using JSON Schema)
	textOutput := p.extractAndCleanTextOutput(resp)
	log.Printf("üì• OPENAI JSON RESPONSE: output_length=%d, output_items=%d, tokens=%d",
		len(textOutput), len(resp.Output), resp.Usage.TotalTokens)

	if textOutput == "" {
		return nil, fmt.Errorf("openai response did not include any output text")
	}

	// Log usage stats
	p.logUsageStats(resp.Usage)

	duration := time.Since(startTime)
	log.Printf("‚úÖ OPENAI GENERATION COMPLETED in %v", duration)

	return &GenerationResponse{
		RawOutput: textOutput, // JSON string from OutputSchema
		Usage:     resp.Usage,
	}, nil
}

// processResponsePlainText extracts plain text output from OpenAI response (no schema/grammar)
// This is useful for simple tasks like generating descriptions
func (p *OpenAIProvider) processResponsePlainText(
	resp *responses.Response,
	startTime time.Time,
	transaction *sentry.Span,
) (*GenerationResponse, error) {
	span := transaction.StartChild("process_response_plaintext")
	defer span.Finish()

	// Extract text output
	textOutput := p.extractAndCleanTextOutput(resp)
	log.Printf("üì• OPENAI PLAIN TEXT RESPONSE: output_length=%d, tokens=%d",
		len(textOutput), resp.Usage.TotalTokens)

	if textOutput == "" {
		return nil, fmt.Errorf("openai response did not include any output text")
	}

	// Log usage stats
	p.logUsageStats(resp.Usage)

	duration := time.Since(startTime)
	log.Printf("‚úÖ OPENAI PLAIN TEXT COMPLETED in %v", duration)

	return &GenerationResponse{
		RawOutput: textOutput,
		Usage:     resp.Usage,
	}, nil
}

// analyzeMCPUsage checks if MCP was used and returns usage details
func (p *OpenAIProvider) analyzeMCPUsage(resp *responses.Response) (bool, int, []string) {
	mcpUsed := false
	mcpCallCount := 0
	toolsUsed := make(map[string]bool)

	log.Printf("üîç MCP USAGE ANALYSIS:")

	for _, outputItem := range resp.Output {
		if outputItem.Type == mcpCallType {
			mcpCall := outputItem.AsMcpCall()
			mcpUsed = true
			mcpCallCount++
			p.logMCPToolCall(mcpCall)
			toolsUsed[mcpCall.Name] = true

			// Add Sentry breadcrumb
			sentry.AddBreadcrumb(&sentry.Breadcrumb{
				Category: "mcp",
				Message:  fmt.Sprintf("MCP tool called: %s", mcpCall.Name),
				Level:    sentry.LevelInfo,
				Data: map[string]interface{}{
					"tool_name":     mcpCall.Name,
					"server_label":  mcpCall.ServerLabel,
					"has_output":    mcpCall.Output != "",
					"output_length": len(mcpCall.Output),
					"has_error":     mcpCall.Error != "",
				},
			})
		}
	}

	uniqueTools := make([]string, 0, len(toolsUsed))
	for tool := range toolsUsed {
		uniqueTools = append(uniqueTools, tool)
	}

	if mcpCallCount == 0 {
		log.Printf("‚ùå MCP NOT USED: No MCP tool calls found in output")
	} else {
		log.Printf("üìä MCP TOOLS USED: %v", uniqueTools)
	}

	return mcpUsed, mcpCallCount, uniqueTools
}

// logMCPToolCall logs details of an MCP tool call
func (p *OpenAIProvider) logMCPToolCall(mcpCall responses.ResponseOutputItemMcpCall) {
	log.Printf("‚úÖ MCP WAS USED: Tool call made")
	log.Printf("   üõ†Ô∏è  Tool Call: %s", mcpCall.Name)
	if mcpCall.Arguments != "" {
		argsStr := mcpCall.Arguments
		if len(argsStr) > maxArgsLogLength {
			argsStr = argsStr[:maxArgsLogLength] + "..."
		}
		log.Printf("     Arguments: %s", argsStr)
	}
	if mcpCall.Output != "" {
		output := mcpCall.Output
		if len(output) > maxOutputTrunc {
			output = output[:maxOutputTrunc] + "... (truncated)"
		}
		log.Printf("     Output: %s", output)
	}
	if mcpCall.Error != "" {
		log.Printf("     ‚ö†Ô∏è  Error: %s", mcpCall.Error)
	}
}

// extractUsageFromRawResponse extracts usage from raw JSON response
func (p *OpenAIProvider) extractUsageFromRawResponse(rawResponse map[string]any) any {
	if usageMap, ok := rawResponse["usage"].(map[string]any); ok {
		return usageMap
	}
	return nil
}

// logUsageStats logs token usage statistics
func (p *OpenAIProvider) logUsageStats(usage responses.ResponseUsage) {
	reasoningTokens := int64(0)
	if usage.OutputTokensDetails.ReasoningTokens > 0 {
		reasoningTokens = usage.OutputTokensDetails.ReasoningTokens
	}
	log.Printf("üìä USAGE: input=%d, output=%d, reasoning=%d, total=%d",
		usage.InputTokens, usage.OutputTokens,
		reasoningTokens, usage.TotalTokens)
}

// logMCPSummary logs a summary of MCP usage
func (p *OpenAIProvider) logMCPSummary(mcpUsed bool, callCount int, tools []string) {
	if mcpUsed {
		log.Printf("üéØ MCP USAGE: %d calls to tools: %v", callCount, tools)
	} else {
		log.Printf("‚ÑπÔ∏è  NO MCP USAGE in this generation")
	}
}

// truncate truncates a string to maxLen characters
func truncate(s string, maxLen int) string {
	if len(s) <= maxLen {
		return s
	}
	return s[:maxLen] + "..."
}

// GenerateStream implements streaming generation using OpenAI's Responses API
// It streams text chunks as they arrive from the LLM and calls the callback for each chunk
func (p *OpenAIProvider) GenerateStream(
	ctx context.Context,
	request *GenerationRequest,
	callback StreamCallback,
) (*GenerationResponse, error) {
	startTime := time.Now()
	log.Printf("üéµ OPENAI STREAMING GENERATION REQUEST STARTED (Model: %s)", request.Model)

	// Start Sentry transaction
	transaction := sentry.StartTransaction(ctx, "openai.generate_stream")
	defer transaction.Finish()

	transaction.SetTag("model", request.Model)
	transaction.SetTag("provider", "openai")
	transaction.SetTag("streaming", "true")

	// Build OpenAI-specific request parameters
	params := p.buildRequestParams(request)

	// Send initial event
	if callback != nil {
		_ = callback(StreamEvent{Type: "started", Message: "Starting generation..."})
	}

	log.Printf("üöÄ OPENAI STREAMING REQUEST: model=%s", request.Model)

	// Call OpenAI streaming API
	span := transaction.StartChild("openai.api_stream")
	stream := p.client.Responses.NewStreaming(ctx, params)
	defer stream.Close()

	// Accumulate text and track usage
	var accumulatedText string
	var finalResponse *responses.Response
	eventCount := 0

	// Process stream events
	for stream.Next() {
		event := stream.Current()
		eventCount++

		// Log event type for debugging (first few events only)
		if eventCount <= maxLogEventCountOpenAI {
			log.Printf("üì• Stream event #%d: type=%s", eventCount, event.Type)
		}

		// Handle different event types
		switch event.Type {
		case "response.output_text.delta":
			// Text delta - this is what we want to stream
			// Use AsResponseOutputTextDelta() to get the properly typed event
			textDelta := event.AsResponseOutputTextDelta()
			delta := textDelta.Delta
			if delta != "" {
				accumulatedText += delta
				if callback != nil {
					_ = callback(StreamEvent{
						Type:    "text_delta",
						Message: delta,
						Data: map[string]interface{}{
							"accumulated_length": len(accumulatedText),
						},
					})
				}
			}

		case "response.output_text.done":
			// Text output complete
			log.Printf("‚úÖ Text output complete: %d chars accumulated", len(accumulatedText))

		case "response.completed":
			// Response complete - extract final response
			completedEvent := event.AsResponseCompleted()
			finalResponse = &completedEvent.Response
			log.Printf("‚úÖ Response completed")

		case "response.failed":
			// Handle failure
			failedEvent := event.AsResponseFailed()
			log.Printf("‚ùå Stream failed: %s", failedEvent.Response.Error.Message)
			span.Finish()
			transaction.SetTag("success", "false")
			return nil, fmt.Errorf("streaming failed: %s", failedEvent.Response.Error.Message)

		case "error":
			// Handle error event
			errorEvent := event.AsError()
			log.Printf("‚ùå Stream error: %s", errorEvent.Message)
			span.Finish()
			transaction.SetTag("success", "false")
			return nil, fmt.Errorf("stream error: %s", errorEvent.Message)

		case "response.function_call_arguments.delta":
			// CFG tool call arguments streaming (for DSL output)
			delta := event.Arguments
			if delta != "" {
				accumulatedText += delta
				if callback != nil {
					_ = callback(StreamEvent{
						Type:    "text_delta",
						Message: delta,
						Data: map[string]interface{}{
							"accumulated_length": len(accumulatedText),
							"is_tool_call":       true,
						},
					})
				}
			}

		case "response.function_call_arguments.done":
			// Tool call arguments complete
			log.Printf("‚úÖ Tool call arguments complete: %d chars", len(accumulatedText))

		default:
			// Log other event types for debugging
			if eventCount <= maxLogEventCountOpenAI {
				log.Printf("üìã Other event type: %s", event.Type)
			}
		}

		// Send periodic heartbeat
		if eventCount%50 == 0 {
			elapsed := time.Since(startTime)
			if callback != nil {
				_ = callback(StreamEvent{
					Type:    "heartbeat",
					Message: "Processing...",
					Data: map[string]interface{}{
						"events_received": eventCount,
						"elapsed_seconds": int(elapsed.Seconds()),
					},
				})
			}
		}
	}

	span.Finish()

	// Check for stream error
	if err := stream.Err(); err != nil {
		log.Printf("‚ùå Stream error: %v", err)
		transaction.SetTag("success", "false")
		sentry.CaptureException(err)
		return nil, fmt.Errorf("stream error: %w", err)
	}

	// Log completion
	duration := time.Since(startTime)
	log.Printf("‚úÖ OPENAI STREAMING COMPLETE: %d events, %d chars, %v duration",
		eventCount, len(accumulatedText), duration)

	// Send completion event
	if callback != nil {
		_ = callback(StreamEvent{
			Type:    "completed",
			Message: "Generation complete",
			Data: map[string]interface{}{
				"total_length": len(accumulatedText),
				"event_count":  eventCount,
			},
		})
	}

	// Build response
	response := &GenerationResponse{
		RawOutput: accumulatedText,
	}

	// Extract usage from final response if available
	if finalResponse != nil {
		response.Usage = finalResponse.Usage
		p.logUsageStats(finalResponse.Usage)
	}

	transaction.SetTag("success", "true")
	return response, nil
}
